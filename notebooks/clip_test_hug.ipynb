{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "84fd90c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/magnus/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-05-18 18:51:40.188640: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-05-18 18:51:40.329880: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-05-18 18:51:41.666607: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import torchaudio\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "import os\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "68658d10",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    }
   ],
   "source": [
    "# Load CLIP model\n",
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "def audio_to_spectrogram(audio_path, save_path=None):\n",
    "    # Load audio\n",
    "    waveform, sample_rate = torchaudio.load(audio_path)\n",
    "    \n",
    "    # Create spectrogram\n",
    "    spectrogram = torchaudio.transforms.Spectrogram()(waveform)\n",
    "    \n",
    "    # Convert to image\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.imshow(spectrogram[0].log2().numpy(), aspect='auto')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, bbox_inches='tight', pad_inches=0)\n",
    "        plt.close()\n",
    "        return save_path\n",
    "    else:\n",
    "        # Convert plot to image\n",
    "        plt.tight_layout(pad=0)\n",
    "        plt.close()\n",
    "        return plt\n",
    "\n",
    "def classify_drum_sound(audio_path, drum_categories, spectrogram_file=\"temp_spectrogram.jpg\"):\n",
    "    # Convert audio to spectrogram image\n",
    "    spectrogram_path = audio_to_spectrogram(audio_path, spectrogram_file)\n",
    "    \n",
    "    # Load image\n",
    "    image = Image.open(spectrogram_path)\n",
    "    \n",
    "    # Prepare text inputs\n",
    "    texts = [f\"A spectrogram of a {category} sound.\" for category in drum_categories]\n",
    "    \n",
    "    # Process inputs\n",
    "    inputs = processor(text=texts, images=image, return_tensors=\"pt\", padding=True)\n",
    "    \n",
    "    # Get similarity scores\n",
    "    outputs = model(**inputs)\n",
    "    logits_per_image = outputs.logits_per_image\n",
    "    probs = logits_per_image.softmax(dim=1)\n",
    "    \n",
    "    # Return prediction\n",
    "    return drum_categories[probs.argmax().item()], probs.tolist()[0]\n",
    "\n",
    "def wav_to_jpg_filename(path_with_filename):\n",
    "    base = os.path.basename(path_with_filename)         # \"snare_1.wav\"\n",
    "    name, _ = os.path.splitext(base)                    # (\"snare_1\", \".wav\")\n",
    "    return f\"{name}.jpg\"                                # \"snare_1.jpg\"\n",
    "\n",
    "\n",
    "def get_wav_files_and_labels(root_dir):\n",
    "    \"\"\"\n",
    "    Recursively fetch all .wav files and assign labels based on the parent directory name.\n",
    "\n",
    "    Args:\n",
    "        root_dir (str or Path): Root directory to search for .wav files.\n",
    "\n",
    "    Returns:\n",
    "        List of tuples: (filepath, label)\n",
    "    \"\"\"\n",
    "    root = Path(root_dir)\n",
    "    files_with_labels = []\n",
    "\n",
    "    for wav_file in root.rglob('*.wav'):\n",
    "        label = wav_file.parent.name\n",
    "        files_with_labels.append((str(wav_file), label))\n",
    "\n",
    "    return files_with_labels\n",
    "\n",
    "def find_tuple_string(array, target_string):\n",
    "    \"\"\"\n",
    "    Searches for a target string in the first element of tuples in an array.\n",
    "    If found, returns the second element of the tuple.\n",
    "\n",
    "    Args:\n",
    "        array (list): A list of tuples.\n",
    "        target_string (str): The string to search for.\n",
    "\n",
    "    Returns:\n",
    "        any: The second element of the tuple if the string is found, otherwise None.\n",
    "    \"\"\"\n",
    "    for tup in array:\n",
    "        if tup[0] == target_string:\n",
    "            return tup[1]\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a32d1457",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating names for CLIP + mapping categories\n",
    "drum_mapping_categories = [\n",
    "                    (\"snare drum\",      \"snare\"), \n",
    "                    (\"bass drum\",       \"bass\"),\n",
    "                    (\"hi-hat cymbal\",   \"hihat_closed\"),\n",
    "                    (\"hi-hat cymbal\",   \"hihat_open\"),\n",
    "                    (\"tom-tom drum\",    \"tomtom_lo\"),\n",
    "                    (\"tom-tom drum\",    \"tomtom_mid\"),\n",
    "                    (\"tom-tom drum\",    \"tomtom_hi\"),\n",
    "                    (\"cymbal\",          \"cymbal\")\n",
    "                ]\n",
    "\n",
    "drum_categories_for_clip = sorted({item[0] for item in drum_mapping_categories})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ad3657",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error --> tomtom: ../data/samples/tomtom/tomtom_hi_3.wav -> snare drum (expected snare)\n",
      "Error --> tomtom: ../data/samples/tomtom/tomtom_low_1.wav -> bass drum (expected bass)\n",
      "Error --> tomtom: ../data/samples/tomtom/tomtom_low_3.wav -> snare drum (expected snare)\n",
      "Error --> tomtom: ../data/samples/tomtom/tomtom_hi_1.wav -> bass drum (expected bass)\n",
      "Error --> tomtom: ../data/samples/tomtom/tomtom_low_2.wav -> bass drum (expected bass)\n",
      "Error --> tomtom: ../data/samples/tomtom/tomtom_mid_2.wav -> snare drum (expected snare)\n",
      "Error --> tomtom: ../data/samples/tomtom/tomtom_mid_3.wav -> tom-tom drum (expected tomtom_lo)\n",
      "Error --> tomtom: ../data/samples/tomtom/tomtom_mid_1.wav -> bass drum (expected bass)\n",
      "Error --> tomtom: ../data/samples/tomtom/tomtom_hi_2.wav -> bass drum (expected bass)\n",
      "Error --> kick: ../data/samples/kick/kick_2.wav -> tom-tom drum (expected tomtom_lo)\n",
      "Error --> kick: ../data/samples/kick/kick_3.wav -> bass drum (expected bass)\n",
      "Error --> kick: ../data/samples/kick/kick_5.wav -> tom-tom drum (expected tomtom_lo)\n",
      "Error --> kick: ../data/samples/kick/kick_1.wav -> bass drum (expected bass)\n",
      "Error --> kick: ../data/samples/kick/kick_4.wav -> tom-tom drum (expected tomtom_lo)\n",
      "Error --> cymbal: ../data/samples/cymbal/cymbal_3.wav -> bass drum (expected bass)\n",
      "Error --> cymbal: ../data/samples/cymbal/cymbal_1.wav -> tom-tom drum (expected tomtom_lo)\n",
      "Error --> cymbal: ../data/samples/cymbal/cymbal_4.wav -> tom-tom drum (expected tomtom_lo)\n",
      "Error --> cymbal: ../data/samples/cymbal/cymbal_2.wav -> tom-tom drum (expected tomtom_lo)\n",
      "Correct --> snare: ../data/samples/snare/snare_5.wav -> snare drum\n",
      "Error --> snare: ../data/samples/snare/snare_7.wav -> tom-tom drum (expected tomtom_lo)\n",
      "Error --> snare: ../data/samples/snare/snare_3.wav -> tom-tom drum (expected tomtom_lo)\n",
      "Error --> snare: ../data/samples/snare/snare_4.wav -> tom-tom drum (expected tomtom_lo)\n",
      "Error --> snare: ../data/samples/snare/snare_2.wav -> tom-tom drum (expected tomtom_lo)\n",
      "Error --> snare: ../data/samples/snare/snare_6.wav -> tom-tom drum (expected tomtom_lo)\n",
      "Error --> snare: ../data/samples/snare/snare_1.wav -> tom-tom drum (expected tomtom_lo)\n",
      "Error --> hihat: ../data/samples/hihat/hihat_closed_6.wav -> tom-tom drum (expected tomtom_lo)\n",
      "Error --> hihat: ../data/samples/hihat/hihat_closed_7.wav -> tom-tom drum (expected tomtom_lo)\n",
      "Error --> hihat: ../data/samples/hihat/hihat_closed_8.wav -> tom-tom drum (expected tomtom_lo)\n",
      "Error --> hihat: ../data/samples/hihat/hihat_open_3.wav -> tom-tom drum (expected tomtom_lo)\n",
      "Error --> hihat: ../data/samples/hihat/hihat_open_2.wav -> tom-tom drum (expected tomtom_lo)\n",
      "Error --> hihat: ../data/samples/hihat/hihat_closed_2.wav -> tom-tom drum (expected tomtom_lo)\n",
      "Error --> hihat: ../data/samples/hihat/hihat_closed_4.wav -> bass drum (expected bass)\n",
      "Error --> hihat: ../data/samples/hihat/hihat_closed_1.wav -> tom-tom drum (expected tomtom_lo)\n",
      "Error --> hihat: ../data/samples/hihat/hihat_open_1.wav -> tom-tom drum (expected tomtom_lo)\n",
      "Error --> hihat: ../data/samples/hihat/hihat_closed_3.wav -> bass drum (expected bass)\n",
      "Error --> hihat: ../data/samples/hihat/hihat_closed_5.wav -> tom-tom drum (expected tomtom_lo)\n",
      "Correct: 1, Incorrect: 35\n",
      "Dog/Cat: ../data/pictures/dog/dog_1.png -> [0.7796632051467896, 0.00041752445395104587, 0.2176366001367569, 0.002282707253471017]\n",
      "Predicted: a photo of a dog\n",
      "Dog/Cat: ../data/pictures/dog/dog_2.png -> [0.9427497386932373, 0.0010284094605594873, 0.03532809019088745, 0.02089378610253334]\n",
      "Predicted: a photo of a dog\n",
      "Dog/Cat: ../data/pictures/dog/dog_3.png -> [0.8644043207168579, 0.00043904597987420857, 0.1337074488401413, 0.0014491911279037595]\n",
      "Predicted: a photo of a dog\n",
      "Dog/Cat: ../data/pictures/cat/cat_1.png -> [0.0017357717733830214, 0.985767662525177, 0.003589848754927516, 0.008906610310077667]\n",
      "Predicted: a photo of a cat\n",
      "Dog/Cat: ../data/pictures/cat/cat_2.png -> [0.004445327911525965, 0.9734030961990356, 0.015313501469790936, 0.006838087923824787]\n",
      "Predicted: a photo of a cat\n",
      "Dog/Cat: ../data/pictures/cat/cat_3.png -> [0.003926614299416542, 0.9808810949325562, 0.011071431450545788, 0.004120876081287861]\n",
      "Predicted: a photo of a cat\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "incorrect = 0\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "os.makedirs(\"../out/clip_test_hug\", exist_ok=True)\n",
    "\n",
    "files_and_labels = get_wav_files_and_labels('../data/samples/')\n",
    "\n",
    "for drum_sample_wav, label in files_and_labels:\n",
    "    drum_sample_jpg = \"../out/clip_test_hug/\" + wav_to_jpg_filename(drum_sample_wav)\n",
    "    classification = classify_drum_sound(drum_sample_wav, drum_categories_for_clip, drum_sample_jpg)\n",
    "\n",
    "    mapped_value = find_tuple_string(drum_mapping_categories, classification[0])\n",
    "\n",
    "    if mapped_value == label:\n",
    "        correct += 1\n",
    "        print(f\"Correct --> {label}: {drum_sample_wav} -> {classification[0]}\")\n",
    "    else:\n",
    "        incorrect += 1\n",
    "        print(f\"Error --> {label}: {drum_sample_wav} -> {classification[0]} (expected {mapped_value})\")\n",
    "\n",
    "print(f\"Correct: {correct}, Incorrect: {incorrect}\")\n",
    "\n",
    "# Validation - How about dogs and cats?\n",
    "dogs_cats = ['../data/pictures/dog/dog_1.png',\n",
    "              '../data/pictures/dog/dog_2.png', \n",
    "              '../data/pictures/dog/dog_3.png', \n",
    "              '../data/pictures/cat/cat_1.png', \n",
    "              '../data/pictures/cat/cat_2.png',\n",
    "              '../data/pictures/cat/cat_3.png']\n",
    "\n",
    "for dog_cat in dogs_cats:\n",
    "    image = Image.open(dog_cat)\n",
    "    texts = [\"a photo of a dog\", \"a photo of a cat\", \"a photo of a cat and dog\", \"a photo of Novia\"]\n",
    "    \n",
    "    inputs = processor(text=texts, images=image, return_tensors=\"pt\", padding=True)\n",
    "    \n",
    "    outputs = model(**inputs)\n",
    "    logits_per_image = outputs.logits_per_image\n",
    "    probs = logits_per_image.softmax(dim=1)\n",
    "    \n",
    "    print(f\"Dog/Cat: {dog_cat} -> {probs.tolist()[0]}\")\n",
    "\n",
    "    predicted_index = probs.argmax().item()\n",
    "    predicted_text = texts[predicted_index]\n",
    "    print(f\"Predicted: {predicted_text}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
